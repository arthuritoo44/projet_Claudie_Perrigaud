# **Projet: Analyse et Visualisation des Donn√©es Web et R√©seaux Sociaux pour le D√©veloppement de l'Activit√© de Sculptrice de Claudie Perrigaud** üé®

Ce GitHub contient la totalit√© des versions permettant la mise en place du processus ETL n√©cessaire √† la valorisation des donn√©es du site internet et des r√©seaux sociaux de Claudie Perrigaud, artiste sculptrice. √Ä noter que le projet est actuellement en cours de d√©veloppement et va √™tre amen√© √† √©voluer continuellement.

![GitHub issues](https://img.shields.io/github/issues/arthuritoo44/projet_Claudie_Perrigaud)
![GitHub forks](https://img.shields.io/github/forks/arthuritoo44/projet_Claudie_Perrigaud)
![GitHub stars](https://img.shields.io/github/stars/arthuritoo44/projet_Claudie_Perrigaud)

## üõ†Ô∏è Pr√©requis

- **Python 3.x**
- **Biblioth√®ques Python** : Prefect, pandas, pymysql, google-auth, google-api-python-client
- **Acc√®s √† Google Analytics API**
- **Base de donn√©es MySQL**

## Installation

1. Clonez ce d√©p√¥t :
   ```bash
   git clone https://github.com/arthuritoo44/projet_Claudie_Perrigaud.git


## Ex√©cution
1. Placez les fichiers `extraction_donnees_reseaux.csv` et `extraction_donnees_site.csv` dans le m√™me r√©pertoire que le script `data_pipeline.py`.
2. Ex√©cutez le script avec la commande suivante :
   python data_pipeline.py
3. Les r√©sultats seront sauvegard√©s dans la base de donn√©es MySQL

## Sommaire

- [1. Introduction](#1-introduction)
- [1.1. Objectif du Projet](#11-objectif-du-projet)
- [1.2. Approche du Projet](#12-approche-du-projet)
- [1.3. D√©roul√© du projet](#13-d√©roul√©-du-projet)
- [2. Mise en place du pipeline de donn√©es](#2-mise-en-place-du-pipeline-de-donn√©es)
- [2.1. Extraction des donn√©es](#21-extraction-des-donn√©es)
- [2.2. Transformation des donn√©es](#22-transformation-des-donn√©es)
- [2.3. Fusion des donn√©es](#23-fusion-des-donn√©es)
- [2.4. Chargement des donn√©es](#24-chargement-des-donn√©es)
- [2.5. Ex√©cution du pipeline](#25-ex√©cution-du-pipeline)

## **1. Introduction**

### **1.1 Objectif du Projet**
Le principal objectif est de **mettre en place un processus ETL** (Extract, Transform, Load) pour int√©grer et analyser les donn√©es provenant de son site Web et de ses r√©seaux sociaux. Il sera crucial de s√©lectionner uniquement les donn√©es importantes afin de les exploiter par la suite.  Cette analyse sera visualis√©e √† l'aide de Power BI pour fournir des insights exploitables qui aideront √† optimiser les strat√©gies de marketing et √† am√©liorer l'exp√©rience utilisateur. Le projet inclura √©galement l'utilisation de Git pour le suivi des modifications du code et des processus.

### **1.2 Approche du Projet**
#### Extraction des Donn√©es:
Site Web: Utilisation de l'API Google Analytics pour extraire les donn√©es de trafic du site Web. Les donn√©es extraites incluront des m√©triques telles que les sessions, les utilisateurs, les vues de pages, la dur√©e moyenne des sessions, et le taux de rebond.
R√©seaux Sociaux: Pr√©paration pour l'extraction des donn√©es des r√©seaux sociaux, en particulier Instagram, √† l'aide des API disponibles pour obtenir des m√©triques telles que les interactions, les mentions, et les performances des publications.


#### Transformation des Donn√©es:
Nettoyage des Donn√©es: Traitement des donn√©es extraites pour √©liminer les valeurs manquantes et les anomalies.
Normalisation des donn√©es pour assurer la coh√©rence entre les diff√©rentes sources.
Combinaison des Donn√©es: Fusion des donn√©es du site Web et des r√©seaux sociaux pour obtenir une vue d'ensemble compl√®te des performances en ligne et ainsi pouvoir √©tablir une strat√©gie globale.

#### Chargement des Donn√©es:
Base de Donn√©es: Chargement des donn√©es nettoy√©es et combin√©es dans MySQL pour une analyse ult√©rieure.
Power BI: Int√©gration des donn√©es dans Power BI pour cr√©er des tableaux de bord et des visualisations interactives.
Versionnage avec Git:
Suivi des Modifications: Utilisation de Git pour versionner les scripts ETL, les transformations de donn√©es et les configurations Power BI. Cela permet de suivre les modifications, de collaborer efficacement et de g√©rer les versions du projet.

#### Visualisation des R√©sultats:
Power BI: Cr√©ation de tableaux de bord dans Power BI pour visualiser les tendances, les performances des pages, les interactions sur les r√©seaux sociaux, et les donn√©es combin√©es. Les visualisations permettront de prendre des d√©cisions bas√©es sur des donn√©es concr√®tes et d'optimiser les strat√©gies en ligne.

#### B√©n√©fices Attendus:
Vue d‚ÄôEnsemble Int√©gr√©e: Une vision compl√®te des performances du site Web et des r√©seaux sociaux, facilit√©e par la combinaison des donn√©es.
Optimisation des Strat√©gies: Am√©lioration des strat√©gies de publication et de marketing gr√¢ce √† des insights bas√©s sur des donn√©es int√©gr√©es et visualis√©es.
Suivi des Modifications: Gestion efficace des modifications et des am√©liorations du code et des processus gr√¢ce au versionnage Git.
Prise de D√©cision √âclair√©e: Capacit√©s de prise de d√©cision bas√©es sur des visualisations claires et des analyses approfondies dans Power BI.

#### Explication des donn√©es
Ce projet a pour objectif de mettre en place un processus ETL qui permettra dans un second temps d‚Äôanalyser les donn√©es afin de produire des insights. Avant toute chose, il semble n√©cessaire d‚Äôexpliquer bri√®vement chaque donn√©e que nous allons √©tudier dans ce projet. A noter que ce projet est amen√© √† √©voluer en fonction des am√©liorations envisageables ainsi que les probl√©matiques et les questionnements apport√©s par l‚Äôartiste.

##### Explication des colonnes de Donn√©es des R√©seaux Sociaux :
**Couverture Facebook/Instagram**
Nombre total de personnes qui ont vu le contenu sur Facebook/Instagram.
**Visites Facebook/Instagram**
Nombre de visites sur la page Facebook/Instagram.
**Interactions Facebook/Instagram**
Nombre total d'interactions sur les publications Facebook/Instagram (likes, commentaires, partages).

##### Explication des colonnes de Donn√©es du Site Web :
**Sessions**
Nombre de sessions (visites) sur le site web.
**Total Users**
Nombre total d'utilisateurs distincts qui ont visit√© le site web.
**Screen Page Views**
Nombre total de pages vues (√©crans) sur le site web.
**Average Session Duration**
Dur√©e moyenne des sessions en secondes.
**Bounce Rate**
Taux de rebond, repr√©sentant le pourcentage de sessions o√π les utilisateurs quittent le site web apr√®s avoir vu une seule page.
**Engaged Sessions**
Nombre de sessions o√π les utilisateurs ont interagi de mani√®re significative avec le site web (par exemple, pages visit√©es).
**New Users**
Nombre d'utilisateurs qui visitent le site web pour la premi√®re fois.
**Event Count**
Nombre total d'√©v√©nements enregistr√©s sur le site web (comme les clics sur des boutons, les soumissions de formulaires, etc.).

### **1.3 D√©roul√© du projet**

Ce projet va √™tre construit autour d‚Äôune pipeline de donn√©es et ceci s‚Äôexplique par diff√©rentes raisons. Tout d‚Äôabord, il implique l'int√©gration de donn√©es provenant de plusieurs sources (site web et r√©seaux sociaux), il est donc n√©cessaire de mettre plusieurs processus en place permettant le recueil, la transformation ainsi que la fusion des donn√©es. 
Voici les raisons principales pour lesquelles une pipeline a √©t√© mise en place. Tout d‚Äôabord, cela va permettre d‚Äôautomatiser certaines t√¢ches r√©p√©titives et de r√©aliser une ex√©cution de ce script √† intervalle r√©gulier. Dans notre cas, l‚Äôintervalle choisi est mensuel, ceci s‚Äôexplique par la charge de donn√©es qui reste encore limit√©e. De plus, une analyse mensuelle semble √™tre un bon compromis afin d‚Äô√©valuer les r√©sultats des diff√©rents ajustements mis en place afin d‚Äôam√©liorer le trafic du site internet et des r√©seaux sociaux de l‚Äôartiste. Ensuite, la pipeline de donn√©es assure la coh√©rence et la standardisation des donn√©es du fait que chaque fois que le script sera ex√©cut√© les m√™mes transformations seront apport√©es que celle de l'√©chantillon pr√©c√©dent. Enfin, cela permet d‚Äôavoir un suivi sur la gestion des √©checs afin de comprendre rapidement d'o√π provient le probl√®me. Pour ce qui est de l‚Äô√©volutivit√© du projet, il sera plus facile de rajouter des donn√©es avec un pipeline de donn√©es bien construit. 
A noter que le choix de Prefect a √©t√© privil√©gi√© plut√¥t qu'Apache Airflow en raison des contraintes sp√©cifiques li√©e √† l‚Äôutilisation d‚ÄôAirflow avec l'environnement Windows. Prefect a ainsi √©t√© choisi comme alternative notamment en raison de sa simplicit√© d'utilisation et de sa compatibilit√© avec les environnements Windows.

## **2. Mise en place du pipeline de donn√©es**
Cette partie va r√©capituler les diff√©rentes √©tapes et les raisons pour lesquelles elles ont √©t√© mises en ≈ìuvre, ainsi que les choix techniques et les d√©fis rencontr√©s. Avant de cr√©er le script de pipeline complet, chaque partie du processus a √©t√© test√©e s√©par√©ment. Cela permet de v√©rifier le bon fonctionnement de chaque composant et d'identifier les √©ventuelles probl√©matiques pouvant appara√Ætre avant d'int√©grer toutes les √©tapes en un seul flux de travail. Afin de mieux comprendre la fa√ßon dont a √©t√© mis en place le pipeline, vous pouvez ouvrir le script data_pipeline.py qui contient la totalit√© du script qui va √™tre d√©taill√© par la suite. Avant toute chose, d√©crivons bri√®vement les t√¢ches effectu√©es dans ce script.

#### Extraction des Donn√©es :
Le script pour extraire les donn√©es du site web √† partir de Google Analytics a √©t√© test√© pour s'assurer que les donn√©es √©taient correctement r√©cup√©r√©es et sauvegard√©es en format CSV. Cette partie peut √™tre amen√©e √† √©voluer afin d‚Äôint√©grer de nouveaux indicateurs √† l‚Äôanalyse. Pour ce qui est des donn√©es des r√©seaux sociaux, elles sont pour le moment extraites via des fichiers csv.
Transformation des Donn√©es : Les donn√©es manquantes ont √©t√© supprim√©es via la suppression des colonnes repr√©sent√©es. Ceci s‚Äôexplique par le fait que les donn√©es manquantes √©t√© tr√®s importantes dans ces colonnes. Pour ce qui est du type des donn√©es, le format date √©tait diff√©rent entre les deux sources de donn√©es, il a donc √©t√© harmonis√© avant la fusion des donn√©es.

#### Fusion des Donn√©es :
Le script pour fusionner les donn√©es extraites avec celles des r√©seaux sociaux a √©t√© test√© pour v√©rifier que les donn√©es √©taient correctement combin√©es et nettoy√©es. Cette √©tape est cruciale car elle permet de cr√©er une base de donn√©es pr√™te √† l‚Äôanalyse

#### Chargement dans MySQL :
Le script pour ins√©rer les donn√©es dans la base de donn√©es MySQL a √©t√© test√© pour garantir que les donn√©es √©taient correctement ins√©r√©es dans la base de donn√©es sans erreurs.

### **2.1 Extraction des donn√©es**
L'extraction des donn√©es du site web via l'API Google Analytics est essentielle pour obtenir des m√©triques pr√©cises sur la performance du site. √âtant donn√© que l‚Äôabonnement de l‚Äôartiste n‚Äôinclut pas la possibilit√© d‚Äôutiliser les API du site internet (squarespace), le choix s‚Äôest port√© sur google analytics qui est relativement simple d‚Äôinitialisation et d‚Äôutilisation et qui regroupe toutes les m√©triques importantes. Cette √©tape est automatis√©e pour se d√©rouler chaque mois afin de fournir des nouvelles donn√©es fra√Æches pour l'analyse.
```python
@task
def get_google_analytics_data():
    SERVICE_ACCOUNT_FILE = 'C:/Users/arthu/OneDrive/Documents/Projet_Claudie_Perrigaud/projet-claudie-perrigaud-8e14bf43e103.json'
    SCOPES = ['https://www.googleapis.com/auth/analytics.readonly']
    PROPERTY_ID = '452047964'

    credentials = service_account.Credentials.from_service_account_file(
        SERVICE_ACCOUNT_FILE, scopes=SCOPES)
    service = build('analyticsdata', 'v1beta', credentials=credentials)
```

Dans cette premi√®re section, j‚Äôai impl√©ment√© une t√¢che qui extrait les donn√©es de Google Analytics en utilisant l‚ÄôAPI de Google Analytics. Cela permet d'automatiser la collecte des m√©triques du site web de Claudie Perrigaud.

La premi√®re √©tape consiste √† utiliser un fichier de compte de service (SERVICE_ACCOUNT_FILE) pour authentifier l‚Äôacc√®s √† l‚ÄôAPI Google Analytics. Ce fichier est g√©n√©r√© au format json depuis Google Cloud Platform et contient toutes les informations d'identification n√©cessaires pour acc√©der aux donn√©es de Google Analytics. Il y a √©galement Le param√®tre SCOPES qui est d√©fini en readonly ce qui signifie que le script peut uniquement lire les donn√©es pr√©sentes dans Google Analytics. Pour ce qui est de l‚Äôidentification du site internet de Claudie Perrigaud sur Google Analytics, c‚Äôest l‚Äôobjet PROPERTY_ID qui nous en informe. Il s'agit d'un identifiant unique associ√© √† chaque compte Google Analytics, permettant de sp√©cifier le site web dont nous souhaitons extraire les donn√©es. Enfin, on utilise les informations d‚Äôidentification (credentials) pour construire le service API. A noter que m√™me si nous utilisons la version beta, elle contient la totalit√© des outils n√©cessaires au bon fonctionnement de notre projet.

```python
    try:
        response = service.properties().runReport(
            property='properties/' + PROPERTY_ID,
            body={
                'dateRanges': [{'startDate': '30daysAgo', 'endDate': 'today'}],
                'metrics': [
                    {'name': 'sessions'},
                    {'name': 'totalUsers'},
                    {'name': 'screenPageViews'},
                    {'name': 'averageSessionDuration'},
                    {'name': 'bounceRate'},
                    {'name': 'engagedSessions'},
                    {'name': 'newUsers'},
                    {'name': 'eventCount'}
                ],
                'dimensions': [
                    {'name': 'date'}
                ]
            }
        ).execute()

        with open('C:/Users/arthu/OneDrive/Documents/Projet_Claudie_Perrigaud/extraction_donnees_site.csv', mode="w", newline="") as file:
            writer = csv.writer(file)
            header = [dimension['name'] for dimension in response.get('dimensionHeaders', [])] + \
                     [metric['name'] for metric in response.get('metricHeaders', [])]
            writer.writerow(header)
            rows = response.get('rows', [])
            for row in rows:
                data_row = [value.get('value', '') for value in row.get('dimensionValues', [])] + \
                          [value.get('value', '') for value in row.get('metricValues', [])]
                writer.writerow(data_row)

        print("Les donn√©es ont √©t√© export√©es avec succ√®s vers extraction_donnees_site.csv")

    except HttpError as err:
        print(f'Une erreur est survenue: {err}')
```

La deuxi√®me partie d‚Äôextraction des donn√©es du site internet consiste √† renseigner les m√©triques, dimensions, la p√©riode de temps pour ainsi exporter les donn√©es dans un fichier csv.
Nous rentrons ensuite dans le script les diff√©rentes m√©triques que nous souhaitons analyser ainsi que la p√©riode de temps sur laquelle r√©cup√©rer les donn√©es, ici mensuel, √† l‚Äôaide de la cl√© de dictionnaire python suivante: 'dateRanges': [{'startDate': '30daysAgo', 'endDate': 'today'}]. Notre projet contient plusieurs m√©triques pour une seule dimension √† savoir la date.

Il m‚Äôa sembl√© int√©ressant d‚Äôutiliser un bloc try-except afin de g√©rer les erreurs pouvant survenir. Dans le cas ou l‚Äôon fait face √† une erreur, elle sera captur√©e par l‚Äôexception: except HttpError as err: print(f'Une erreur est survenue: {err}')
Le script exporte ensuite les r√©sultats de la requ√™te dans un fichier csv avec en en-t√™te les m√©triques d√©termin√©es pr√©c√©demment. Un message de confirmation mentionnant que l‚Äôexport des donn√©es dans le fichier csv a r√©ussi permet d‚Äôavoir une visibilit√© imm√©diate du r√©sultat de la requ√™te.

### **2.2 Transformation des donn√©es**

La partie transformation et nettoyage des donn√©es a √©t√© directement imbriqu√© avec la seconde t√¢che de fusion des donn√©es. Pour ce qui est des valeurs manquantes, apr√®s √©tudes des donn√©es, nous remarquons qu‚Äôelles peuvent √™tre remplac√©es par 0 au lieu d‚Äô√™tre supprim√©es et ainsi perdre de l‚Äôinformation. Ceci √©tant effectu√©e via cette commande : donnees_fusionnees.fillna(0, inplace=True).
Ensuite, il a √©t√© n√©cessaire d‚Äôharmoniser les dates en leur attribuant le m√™me format via cette fonction :
```python
reseaux['date'] = pd.to_datetime(reseaux['date'], format='%d/%m/%Y') 
site['date'] = pd.to_datetime(site['date'], format='%Y%m%d')
```
Cette fonction Pandas est indispensable afin d‚Äôeffectuer la fusion des deux tables.

## **2.3 Fusion des donn√©es**
```python
@task
def merge_data():
    # Chargement des donn√©es
    reseaux = pd.read_csv('C:/Users/arthu/OneDrive/Documents/Projet_Claudie_Perrigaud/extraction_donnees_reseaux.csv', sep=',')
    site = pd.read_csv('C:/Users/arthu/OneDrive/Documents/Projet_Claudie_Perrigaud/extraction_donnees_site.csv', sep=',')

    # Conversion des colonnes de date au format datetime
    reseaux['date'] = pd.to_datetime(reseaux['date'], format='%d/%m/%Y')
    site['date'] = pd.to_datetime(site['date'], format='%Y%m%d')

    # Fusion des deux DataFrames
    donnees_fusionnees = pd.merge(reseaux, site, on='date', how='outer')
    
    # Suppression des lignes contenant des valeurs manquantes
    # Remplacement des valeurs manquantes par 0
    donnees_fusionnees.fillna(0, inplace=True)

    # Sauvegarde des donn√©es fusionn√©es
    donnees_fusionnees.to_csv('C:/Users/arthu/OneDrive/Documents/Projet_Claudie_Perrigaud/donnees_fusionnees.csv', index=False)

    print("Fusion des donn√©es r√©alis√©e avec succ√®s.")

    return donnees_fusionnees
```
La fonction merge_data est ensuite impl√©ment√©e via Prefect, elle repr√©sente la deuxi√®me √©tape de ce processus ETL. Elle se charge de fusionner les donn√©es provenant des r√©seaux sociaux et du site web avant leur stockage dans la base de donn√©es MySQL. Plusieurs fonctions Pandas sont utilis√©es dans cette t√¢che, la fonction pd.read_csv() permet le chargement des donn√©es des deux sources. La m√©thode pd.merge() est ensuite impl√©ment√©e avec une jointure externe how=‚Äôouter‚Äô, cela signifie que la totalit√© des lignes des deux fichiers seront int√©gr√©es. Pour terminer, le fichier avec les donn√©es fusionn√©es est ensuite sauvegard√© au format csv dans le r√©pertoire du projet.

### **2.4 Chargement des donn√©es**
```python
@task
def save_to_mysql(dataframe):
    # Configuration de la connexion √† MySQL
    connection = pymysql.connect(
        host='localhost',         
        user='root',              
        password='Mecano44!',      
        database='projet_claudie_perrigaud'    
    )
    
    # Nom de table dynamique avec la date et l'heure pour √©viter la suppression des donn√©es
    table_name = "analyse_donnees_" + datetime.now().strftime('%Y%m%d_%H%M%S')
```
La derni√®re √©tape du pipeline de traitement est orchestr√©e autour de la fonction save_to_mysql permettant le stockage dans une base de donn√©es MySQL.
Apr√®s s‚Äô√™tre connect√©e √† la base de donn√©es initialis√©e pour cette table, une table dynamique est cr√©√©e de mani√®re √† ce que les donn√©es pr√©c√©dentes soient sauvegard√©es √† chaque nouvelle ex√©cution. Chaque table portera donc un nom unique g√©n√©r√© √† partir de la date et l‚Äôheure du jour de l‚Äôex√©cution du script. 

```python
    try:
        with connection.cursor() as cursor:
            # Cr√©ation de la table avec toutes les colonnes n√©cessaires
            create_table_query = f"""
            CREATE TABLE IF NOT EXISTS {table_name} (
                date DATE,
                couverture_facebook FLOAT,
                visites_facebook INT,
                interactions_facebook INT,
                couverture_instagram FLOAT,
                interactions_instagram INT,
                visites_instagram INT,
                sessions INT,
                totalUsers INT,
                screenPageViews INT,
                averageSessionDuration FLOAT,
                bounceRate FLOAT,
                engagedSessions INT,
                newUsers INT,
                eventCount INT
            );
            """
            cursor.execute(create_table_query)

            # Insertion des donn√©es
            insert_query = f"""
            INSERT INTO {table_name} (
                date, couverture_facebook, visites_facebook, interactions_facebook,
                couverture_instagram, interactions_instagram, visites_instagram,
                sessions, totalUsers, screenPageViews, averageSessionDuration,
                bounceRate, engagedSessions, newUsers, eventCount
            )
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """
            
            for _, row in dataframe.iterrows():
                cursor.execute(insert_query, tuple(row))

        # Commit des modifications
        connection.commit()
        print("Les donn√©es ont √©t√© enregistr√©es dans la base de donn√©es MySQL avec succ√®s.")

    finally:
        connection.close()
```
La table est ensuite cr√©√© avec toutes les colonnes n√©cessaires pour accueillir les donn√©es. Elles sont ins√©r√©es via une requ√™te SQL INSERT INTO et la m√©thode cursor.execute() parcours chaque ligne du dataframe et ins√®re les valeurs. Les donn√©es ins√©r√©es sont ensuite valid√©es via la commande connection.commit() qui permet l‚Äôenregistrement permanent dans la table MySQL. La connexion est ensuite ferm√©e afin de lib√©rer les ressources et √©viter des connexions inutiles. Cette √©tape permet l‚Äôenregistrement dans une base de donn√©es MySQL avec un syst√®me assez flexible permettant une persistance des donn√©es et un historique bien d√©fini.


### **2.5 Ex√©cution du pipeline**

```python
@flow
def data_pipeline():
    get_google_analytics_data()
    merged_data = merge_data()
    save_to_mysql(merged_data)

# Ex√©cution du flux
if __name__ == "__main__":
    data_pipeline()
```

Pour terminer, les diff√©rentes √©tapes initialis√©es pr√©c√©demment sont int√©gr√©es dans la fonction data_pipeline qui est un outil de gestion de workflows dans Prefect afin d‚Äôautomatiser l'encha√Ænement des diff√©rentes t√¢ches.
A la suite de cela, le bloc final permet de lancer le pipeline de donn√©es d√®s lors que le script est ex√©cut√©.
